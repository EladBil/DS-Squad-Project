{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701b46a3",
   "metadata": {},
   "source": [
    "# Data Science Workshop Project\n",
    "**Team:** Elad Bilman, Oz Hizkia, Eva Hallermeier, Tzach Cohen\n",
    "\n",
    "**Problem:** The prediction of in-hospital mortality for admitted patients remain poorly characterized and is biased by\n",
    "             Doctor's opinion.\n",
    "\n",
    "**Goal of the project**: We aimed to develop and validate a prediction model for all-cause in-hospital mortality among\n",
    "                         admitted patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fef391",
   "metadata": {},
   "source": [
    "## Part 1 - Prologue\n",
    "### Dataset: Patient Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55840b66",
   "metadata": {},
   "source": [
    "We choose a dataset from Kaggle  https://www.kaggle.com/datasets/mitishaagarwal/patient\n",
    "\n",
    "This dataset is a collection of patients that visited the ICU in an hospital. Each row represents a patient, <br>\n",
    "which in itself is a collection of checkups of the patient 1 hour after his reception and 1 day after his reception, <br>\n",
    "among other medical examinations. The data also includes the basic characeristics of the patient such as gender, <br>\n",
    "ethnicity, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93880c33",
   "metadata": {},
   "source": [
    "#### Import Modules, Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aff1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning Library\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Analyze Model Tools\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualization Tools\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from DataprocessingMethods import *\n",
    "from Model.FinalModel import FinalModel\n",
    "from Model.ModelModule import DSWorkshopModel\n",
    "from MissingValuesVisualization import *\n",
    "\n",
    "# QOL Functions\n",
    "from UsefullFunctions import load_dataset\n",
    "from UsefullFunctions import conf_matrix\n",
    "from UsefullFunctions import plotPR\n",
    "from UsefullFunctions import plotRoc\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Original Dataset\n",
    "df = load_dataset()\n",
    "df.head() # Overview of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d03a",
   "metadata": {},
   "source": [
    "The dataset is composed of 85 columns (features) and we have 91,713 patients. <br> \n",
    "We can see that we have an entire empty column (index=83) that we will remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2b25a",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943feaa",
   "metadata": {},
   "source": [
    "We have different types of feature: numerical, categorical and binary. Some of the features are data about <br> the medical measures of the patient and some are description of hospital unit and cares he received. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560895db",
   "metadata": {},
   "source": [
    "<span style='background:#5c97ff;color:Black'>\n",
    "In addition, we wrote a complete file which explains all medical features.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_features = [\"hospital_id\", \"ethnicity\", \"gender\", \"icu_admit_source\", \n",
    "                       \"apache_3j_bodysystem\", \"apache_2_bodysystem\", \"icu_stay_type\", \"icu_type\"]\n",
    "\n",
    "numerical_features = [\"age\", \"bmi\",\"height\", \"weight\", \n",
    "                      \"pre_icu_los_days\", \"gcs_eyes_apache\",\"apache_2_diagnosis\",\n",
    "                      \"gcs_motor_apache\", \"gcs_verbal_apache\", \"heart_rate_apache\",\n",
    "                     \"map_apache\", \"resprate_apache\", \"temp_apache\", \"d1_diasbp_max\",\n",
    "                      \"d1_diasbp_min\",\"d1_diasbp_noninvasive_max\", \"d1_diasbp_noninvasive_min\",\n",
    "                      \"d1_heartrate_max\", \"d1_heartrate_min\", \"d1_mbp_max\", \"d1_mbp_min\", \n",
    "                      \"d1_mbp_noninvasive_max\", \"d1_mbp_noninvasive_min\", \"d1_resprate_max\", \n",
    "                      \"d1_resprate_min\",\"d1_spo2_max\", \"d1_spo2_min\", \"d1_sysbp_max\", \"d1_sysbp_min\",\n",
    "                      \"d1_sysbp_noninvasive_max\", \"d1_sysbp_noninvasive_min\", \"d1_temp_max\", \n",
    "                      \"d1_temp_min\",\"h1_diasbp_max\", \"h1_diasbp_min\", \"h1_diasbp_noninvasive_max\", \n",
    "                      \"h1_diasbp_noninvasive_min\",\"h1_heartrate_max\", \"h1_heartrate_min\", \n",
    "                      \"h1_mbp_max\", \"h1_mbp_min\",\"h1_mbp_noninvasive_max\", \"h1_mbp_noninvasive_min\",\n",
    "                      \"h1_resprate_max\", \"h1_resprate_min\",\"h1_spo2_max\", \"h1_spo2_min\", \n",
    "                      \"h1_sysbp_max\", \"h1_sysbp_min\",\"h1_sysbp_noninvasive_max\", \n",
    "                      \"h1_sysbp_noninvasive_min\", \"d1_glucose_max\", \"d1_glucose_min\",\n",
    "                      \"d1_potassium_max\", \"d1_potassium_min\", \"apache_4a_hospital_death_prob\", \n",
    "                      \"apache_4a_icu_death_prob\",\"apache_3j_diagnosis\"]\n",
    "\n",
    "\n",
    "binary_features = [\"arf_apache\", \"gcs_unable_apache\", \"intubated_apache\", \n",
    "                   \"ventilated_apache\", \"elective_surgery\", \"apache_post_operative\",\n",
    "                   \"aids\", \"cirrhosis\", \"diabetes_mellitus\", \"hepatic_failure\", \"immunosuppression\",\n",
    "                   \"leukemia\", \"lymphoma\", \"solid_tumor_with_metastasis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5eb09",
   "metadata": {},
   "source": [
    "### Part 2.1 - The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacf630",
   "metadata": {},
   "source": [
    "**Info About the Prediction Problem:** <br>\n",
    "We chose to classify wether the patient will \"go into very critical state\" or not as the dataset have a fetaure of wether the <br>\n",
    "patient is alive or not.\n",
    "So our problem is binary classification, if the patient is alive it's marked as 0, otherwise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fb08b",
   "metadata": {},
   "source": [
    "### Part 2.2 - Thankfully Unbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction = \"hospital_death\"\n",
    "unique_labels = np.unique(df[class_prediction])\n",
    "plt.pie(np.array([len(df[df[class_prediction]==label]) for label in unique_labels ]), \n",
    "        labels = list(unique_labels), autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c11eec",
   "metadata": {},
   "source": [
    "As we can see, the difference in quantity of the two classes is exceptionally large, <br>\n",
    "with the label '1' being far less common. <br>\n",
    "Due to that, it will be a challenge for the model to predict the label '1', which represents cases that ended <br>\n",
    "with patients' death.\n",
    "\n",
    "As such we face the challenge of keeping the model from being biased, since most of the data is labeled with '0' <br>\n",
    "which will cause the model to easily predict this label, but not the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a4fb2",
   "metadata": {},
   "source": [
    "## Part 4 - Sanitizing and Organizing the Dataset\n",
    "### Part 4.1 - Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeMissingValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValuesDistribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ee64a",
   "metadata": {},
   "source": [
    "<span style='background:#ffd500;color:Black'> Most of missing values are numerical. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[[0,1,83]], axis=1, inplace=True) # Remove useless column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542373ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "featuresWithManyMissingValues = missing_value_df.sort_values(by='percent_missing',ascending=False).head(25)\n",
    "featuresWithManyMissingValues.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4b1b5",
   "metadata": {},
   "source": [
    "### Part 4.2 - Deal With Missing Values:\n",
    "- For Categorical values:\n",
    "    - We will do one hot encoding so we would not need to deal with missing values. <br>\n",
    "    We wouldn't include in the clean dataset cells with no categorial match and put a meaningless value.\n",
    "- For Numerical values:\n",
    "    - We will fill the cells with the mean of the feature taken <span style='color: #ff0000'> ONLY </span> from the test.\n",
    "- For Binary values: \n",
    "    - We will fill missing values with 0 because most of binary values tell if a patient was given a specific problem <br>\n",
    "    or a specific treatment, we will assume that if its not written that it hasn't happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836782de",
   "metadata": {},
   "source": [
    "### Part 4.3 - Informative Data Visualization## 5- Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eead16",
   "metadata": {},
   "source": [
    "#### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of interesting categorical feature: gender, ethnicity and admission diagnoosis (type of health problem)\n",
    "\n",
    "columns =  [\"gender\", \"ethnicity\",\"apache_3j_bodysystem\"]\n",
    "fig, axes = plt.subplots(1,3,figsize=(12,5))\n",
    "\n",
    "n=len(columns)\n",
    "num_rows = 1\n",
    "max_bars = 8\n",
    "\n",
    "for i,variable in enumerate(columns):\n",
    "    u=min(df[variable].nunique(),max_bars)\n",
    "    vc = df[variable].value_counts()[:u]\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    vc.plot(kind='bar',ax=axes[i],title=variable)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.99, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc358e09",
   "metadata": {},
   "source": [
    "#### Influence of the Age of the Patient on Hospital Death"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4b160",
   "metadata": {},
   "source": [
    "We know that older patient are at a bigger risk of complications and we can see it clearly on this graph. <br>\n",
    "The number of deaths is bigger for older patients. <br>\n",
    "In natural correlation we will also see in the model examination that age is a <b>very</b> important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_patient = df[df[\"hospital_death\"] == 1]\n",
    "living_patient = df[df[\"hospital_death\"] == 0]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,10))\n",
    "\n",
    "#create two histograms\n",
    "sns.distplot(dead_patient.age, bins = 25, kde = True, label = \"dead\",ax=ax)\n",
    "sns.distplot(living_patient.age, bins = 25, kde = True, label = \"living\",ax=ax)\n",
    "\n",
    "plt.title('Hospital death Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1024634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getBasicDataset()\n",
    "#prepare data\n",
    "x_data = df.drop('hospital_death', axis=1)\n",
    "true_values = df.hospital_death\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, true_values, test_size=0.2, stratify=true_values, shuffle=True)\n",
    "x_train = fill_missing_num_values_with_mean(x_train)\n",
    "x_train = fill_missing_values_binary(x_train)\n",
    "x_test = fill_test_missing_num_values_with_mean(x_test,x_train)\n",
    "x_test = fill_test_missing_values_binary(x_test, x_train)\n",
    "\n",
    "all_data = pd.concat([x_train, x_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639ad04",
   "metadata": {},
   "source": [
    "## Part 6 - Running Naive Model <br> for Baseline Results and Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55237a7",
   "metadata": {},
   "source": [
    "We tested a few basic models:\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- XGBoost\n",
    "\n",
    "We get the best result from the XGBoost so we decided to stick with it for our advanced model.\n",
    "<br>See the basic results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init model\n",
    "model = DSWorkshopModel(df) \n",
    "model.set_split(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "\n",
    "#train mondel\n",
    "model.train()\n",
    "\n",
    "#test model\n",
    "models_predictions, pred_results = model.test()\n",
    "\n",
    "#results\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b5ee3",
   "metadata": {},
   "source": [
    "Because we have an unbalanced model we will use \"Balanced Accuracy\" and \"Recall\" as our \"guiding\" metrics.\n",
    "\n",
    "*Note: Balanced Accuarcy is like the Accuracy metric but the skewness of the metrics.\n",
    "\n",
    "**Result of the basic model as baseline:**\n",
    "- Recall around (positive accuracy) 33% so many false negative\n",
    "- Precision around  67%\n",
    "- Global Accuracy 93%\n",
    "- Balanced accuracy around 65%\n",
    "- Negative accuracy around 98 % \n",
    "\n",
    "Because we have only around 10% of death cases in our dataset it means that if we have 90% accuracy <br>\n",
    "thus we probably always give 0 as prediction let's, verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f66eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test.to_numpy(), models_predictions[-1])\n",
    "conf_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813f118",
   "metadata": {},
   "source": [
    "As we predicted the biggest problem is that we have <span style='color: #ff0000'> a lot </span> of false\n",
    "negative predictions so our goal will be to reduce it. <br>\n",
    "\n",
    "<span style='color: #ff0000'> Important: </span>\n",
    "<br>\n",
    "We will prefer to predict the \"worst\" (practicaly increasing the false positives) such that a patient <br> in danger\n",
    "will receive more attention and intensive care which maybe can save him."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38805e3c",
   "metadata": {},
   "source": [
    "### Part 6.1 - Analyze Performance of the Basic Model\n",
    "**Precision Recall-Curve:**\n",
    "Precision-Recall curves should be used when there is a moderate to large class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "xgb = model.get_models()[0] #xgboost\n",
    "predicted_probs = xgb.predict_proba(x_test)\n",
    "y_score = predicted_probs[:,1]\n",
    "    \n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_true, y_score, pos_label=1)\n",
    "plotPR(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114fdf33",
   "metadata": {},
   "source": [
    "**ROC Plot:**<br>\n",
    "ROC curves should be used when there are roughly equal numbers of observations for each class which is not our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = sklearn.metrics.roc_auc_score(y_true, y_score)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true, y_score)\n",
    "plotRoc(fpr, tpr, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d053c",
   "metadata": {},
   "source": [
    "### Part 6.2 - SHAP Analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c54f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(all_data)\n",
    "\n",
    "#feature importance analysis\n",
    "shap.summary_plot(shap_values, all_data, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ef6e0",
   "metadata": {},
   "source": [
    "We can see here clearly here which features have the biggest influence on the prediction of the model.\n",
    "- 1. Hospital Death Probability\n",
    "- 2. Age\n",
    "- 3. Minimun Heartrate after 1 day\n",
    "- 4. Ventilated Apache\n",
    "\n",
    "All of those features makes sense so we want to capitilize on them in the final model.\n",
    "Also as we saw in the graph age is a core player in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ac0fe",
   "metadata": {},
   "source": [
    "## Part 7 - Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73901d4",
   "metadata": {},
   "source": [
    "### Part 7.1 - Removing Higle Effictive Yet Correlated Feature\n",
    "In our analysis of the model we noticed two highly correlated features:\n",
    "- 1. apache_4a_hospital_death_prob\n",
    "- 2. apache_4a_icu_death_prob\n",
    "\n",
    "Below we see the value which is the total number of missing values in each one so we have missing values <br>\n",
    "for same patients in both of the features. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"apache_4a_hospital_death_prob\",\"apache_4a_icu_death_prob\"]].dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d583cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poc_dataset = load_dataset()\n",
    "#they have same quantity of missing values (based on our previous missing value analysis)\n",
    "#We would like to check if missing values are for same patients\n",
    "counter = 0\n",
    "\n",
    "for idx,row in poc_dataset.iterrows():\n",
    "    if pd.isnull(row[\"apache_4a_hospital_death_prob\"]) and pd.isnull(row[\"apache_4a_icu_death_prob\"]):\n",
    "        counter+=1\n",
    "\n",
    "print((counter/poc_dataset.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2727a",
   "metadata": {},
   "source": [
    "Lets now try to check if they have same values or really close:<br>\n",
    "- if yes we don't need both of them and we will keep only one, the one that will get better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for idx,row in poc_dataset.iterrows():\n",
    "    if abs(row[\"apache_4a_hospital_death_prob\"] - row[\"apache_4a_icu_death_prob\"])<0.15:\n",
    "        counter+=1\n",
    "\n",
    "print((counter/poc_dataset.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dba501",
   "metadata": {},
   "source": [
    "85% of the values are really close, so it means that we don't really need both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Original Dataset\n",
    "df = getBasicDataset()\n",
    "\n",
    "# Dropping the Deature\n",
    "df = df.drop('apache_4a_hospital_death_prob', axis=1)\n",
    "\n",
    "\n",
    "# Drop the classification column.\n",
    "x_data = df.drop('hospital_death', axis=1)\n",
    "true_values = df.hospital_death"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfb35c",
   "metadata": {},
   "source": [
    "### Part 7.2 - Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, test.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, true_values, test_size=0.2,\n",
    "    stratify=true_values, shuffle=True)\n",
    "# Fill missing values.\n",
    "x_train = fill_missing_num_values_with_mean(x_train)\n",
    "# Fill missing binary values.\n",
    "x_train = fill_missing_values_binary(x_train)\n",
    "# Fill missing values.\n",
    "x_test = fill_test_missing_num_values_with_mean(x_test,x_train)\n",
    "# Fill missing binary values.\n",
    "x_test = fill_test_missing_values_binary(x_test, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59437b",
   "metadata": {},
   "source": [
    "### Part 7.3 - Oversampling: Make the Training Set More Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f200685",
   "metadata": {},
   "source": [
    "In order to deal with the skewness of the dataset we use an Oversampling algorithm to increase the number of \"1\" <br>\n",
    "in our dataset. <br>\n",
    "We use the RandomOverSampler algorithm we pickes random lines in the class and generates new ones accordingly.<br>\n",
    "*Note: we use this <span style='color: #ff0000'> ONLY </span> on the train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the oversample model.\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_np = x_train.to_numpy()\n",
    "y_np = y_train.to_numpy()\n",
    "# Oversampling the test.\n",
    "x_np, y_np = oversample.fit_resample(x_np, y_np)\n",
    "# Convert back to pandas\n",
    "x_train = pd.DataFrame(x_np, columns=x_train.columns)\n",
    "y_train = pd.Series(y_np, name=y_train.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e40ff0",
   "metadata": {},
   "source": [
    "### Part 7.4 - Running the Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6106be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel(df)\n",
    "model.set_split(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "\n",
    "#train mondel\n",
    "model.train()\n",
    "\n",
    "#test model\n",
    "models_predictions, pred_results = model.test()\n",
    "\n",
    "#results\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaf66f",
   "metadata": {},
   "source": [
    "**Result of the basic model as baseline:**\n",
    "- Recall around (positive accuracy) 81% and was 33% in the basic model!\n",
    "- Balanced accuracy around 77 % and was around 65% in the basic model!\n",
    "- Negative accuracy and unbalanced accuracy (normal one) decrease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae331d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test.to_numpy(), models_predictions[-1])\n",
    "conf_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff385b",
   "metadata": {},
   "source": [
    "We can say now that we now predict better death cases which was our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bda37",
   "metadata": {},
   "source": [
    "### SHAP on the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "all_data = pd.concat([x_train, x_test], axis=0)\n",
    "xgb = model.get_models()[0] #xgboost\n",
    "predicted_probs = xgb.predict_proba(x_test)\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(all_data)\n",
    "\n",
    "#feature importance analysis\n",
    "shap.summary_plot(shap_values, all_data, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea05b08",
   "metadata": {},
   "source": [
    "We can see also that we have as expected high correlation between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3999c",
   "metadata": {},
   "source": [
    "### Run with only \"apache_4a_icu_death_prob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f235ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getBasicDataset()\n",
    "df = df.drop('apache_4a_hospital_death_prob', axis=1)\n",
    "#prepare data\n",
    "x_data = df.drop('hospital_death', axis=1)\n",
    "true_values = df.hospital_death\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, true_values, test_size=0.2, stratify=true_values, shuffle=True)\n",
    "x_train = fill_missing_num_values_with_mean(x_train)\n",
    "x_train = fill_missing_values_binary(x_train)\n",
    "x_test = fill_test_missing_num_values_with_mean(x_test,x_train)\n",
    "x_test = fill_test_missing_values_binary(x_test, x_train)\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_np = x_train.to_numpy()\n",
    "y_np = y_train.to_numpy()\n",
    "x_np, y_np = oversample.fit_resample(x_np, y_np)\n",
    "# Convert back to pandas\n",
    "x_train = pd.DataFrame(x_np, columns=x_train.columns)\n",
    "y_train = pd.Series(y_np, name=y_train.name)\n",
    "\n",
    "\n",
    "model = FinalModel(df)\n",
    "model.set_split(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "\n",
    "#train mondel\n",
    "model.train()\n",
    "\n",
    "#test model\n",
    "models_predictions, pred_results = model.test()\n",
    "\n",
    "#results\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20591684",
   "metadata": {},
   "source": [
    "### Run with only \"apache_4a_hospital_death_prob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f367c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getBasicDataset()\n",
    "df =df.drop('apache_4a_icu_death_prob',axis=1)\n",
    "#prepare data\n",
    "x_data = df.drop('hospital_death', axis=1)\n",
    "true_values = df.hospital_death\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, true_values, test_size=0.2, stratify=true_values, shuffle=True)\n",
    "x_train = fill_missing_num_values_with_mean(x_train)\n",
    "x_train = fill_missing_values_binary(x_train)\n",
    "x_test = fill_test_missing_num_values_with_mean(x_test,x_train)\n",
    "x_test = fill_test_missing_values_binary(x_test, x_train)\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_np = x_train.to_numpy()\n",
    "y_np = y_train.to_numpy()\n",
    "x_np, y_np = oversample.fit_resample(x_np, y_np)\n",
    "# Convert back to pandas\n",
    "x_train = pd.DataFrame(x_np, columns=x_train.columns)\n",
    "y_train = pd.Series(y_np, name=y_train.name)\n",
    "\n",
    "\n",
    "model = FinalModel(df)\n",
    "model.set_split(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "\n",
    "#train mondel\n",
    "model.train()\n",
    "\n",
    "#test model\n",
    "models_predictions, pred_results = model.test()\n",
    "\n",
    "# Results\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d53b0e",
   "metadata": {},
   "source": [
    "We get better performance with only \"apache_4a_hospital_death_prob\" so we will remove for next trials the feature \"apache_4a_icu_death_prob\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f3999",
   "metadata": {},
   "source": [
    "## 7.2 Oversampling with arrangement of data based on importance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c591e0",
   "metadata": {},
   "source": [
    "Lets try to check which features have  an important impact on the model and have many missing values filed which can give bad data to the model.We will build list of features with high importance and many missing values and we will analyze them and decide how to represent them is the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresWithManyMissingValues = featuresWithManyMissingValues[['column_name']]\n",
    "\n",
    "vals= np.abs(shap_values).mean(0)\n",
    "feature_importance = pd.DataFrame(list(zip(all_data.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24519829",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_feature = feature_importance['col_name'].tolist()\n",
    "featuresWithManyMissingValues = featuresWithManyMissingValues['column_name'].tolist()\n",
    "set1 = set(most_important_feature)\n",
    "set2 = set(featuresWithManyMissingValues)\n",
    "newList = list(set1.intersection(set2))\n",
    "print(\"Important features with many missing data originally are:\\n\", newList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f2d7f",
   "metadata": {},
   "source": [
    "About the feature h1_spo2_max and 'h1_spo2_min': it about saturation: percentage of oxygene in the blood: someone in good health has around 100 % and someone which have pulmonary or very sick can have between 80 and 95%. Lets check the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15402e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getBasicDataset()\n",
    "\n",
    "dfspo2max = df['h1_spo2_max'].mean()\n",
    "print(\"mean 'h1_spo2_max' is \", dfspo2max)\n",
    "dfspo2min = df['h1_spo2_min'].mean()\n",
    "print(\"mean 'h1_spo2_min' is \", dfspo2min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e75fd4",
   "metadata": {},
   "source": [
    "For those features for saturation we check that mean of saturation based on dataset are ok for filling missing values because normal saturation is higher than around 96% so we can say that maximum can be 98 % and minimum 95 % : those values are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad680ac8",
   "metadata": {},
   "source": [
    "About d1_potassium_max and d1_potassium_min: let s try to make the same analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getBasicDataset()\n",
    "\n",
    "dfspo2max = df['d1_potassium_max'].mean()\n",
    "print(\"mean 'd1_potassium_max' is \", dfspo2max)\n",
    "dfspo2min = df['d1_potassium_min'].mean()\n",
    "print(\"mean 'd1_potassium_min' is \", dfspo2min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6ec33",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb94a9",
   "metadata": {},
   "source": [
    "We built a model which predict death case for admitted patient at hospital. Our model is sensitive and can predict death even if the patient will probably finish alive but it is preferable this way because we don t want to miss patient that can be not treated as they need in order to stay alive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "398e8141fdf2977bb876422d91cc2aca16ac353583e53b22670e81c1c8f67dd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
